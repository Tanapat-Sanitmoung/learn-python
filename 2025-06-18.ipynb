{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a52f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_start_end(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"Start ....\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"End ....\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c55a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ....\n",
      "adding 1 and 2\n",
      "End ....\n",
      "result is 3\n"
     ]
    }
   ],
   "source": [
    "@log_start_end\n",
    "def add(a, b):\n",
    "    print(f\"adding {a} and {b}\")\n",
    "    return a + b\n",
    "\n",
    "result = add(1, 2)\n",
    "\n",
    "print(f\"result is {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5b3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  ID              "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE      MODIFIED     \n",
      "qwen3:4b                              2bfd38a7daaf    2.6 GB    23 hours ago    \n",
      "deepseek-r1:1.5b                      e0979632db5a    1.1 GB    23 hours ago    \n",
      "mxbai-embed-large:latest              468836162de7    669 MB    2 days ago      \n",
      "qwen2.5-coder:3b                      f72c60cabf62    1.9 GB    8 days ago      \n",
      "scb10x/typhoon2.1-gemma3-4b:latest    8cfab9097c9d    2.6 GB    4 weeks ago     \n",
      "bge-m3:latest                         790764642607    1.2 GB    5 weeks ago     \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "exc_result = os.system(\"ollama list\")\n",
    "\n",
    "exc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbff56f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(exc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a665a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  ID              SIZE      MODIFIED     \n",
      "qwen3:4b                              2bfd38a7daaf    2.6 GB    23 hours ago    \n",
      "deepseek-r1:1.5b                      e0979632db5a    1.1 GB    23 hours ago    \n",
      "mxbai-embed-large:latest              468836162de7    669 MB    2 days ago      \n",
      "qwen2.5-coder:3b                      f72c60cabf62    1.9 GB    8 days ago      \n",
      "scb10x/typhoon2.1-gemma3-4b:latest    8cfab9097c9d    2.6 GB    4 weeks ago     \n",
      "bge-m3:latest                         790764642607    1.2 GB    5 weeks ago     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "sp_result = subprocess.check_output(\n",
    "    \"ollama list\", \n",
    "    shell=True, \n",
    "    text=True, # ensure result will return as string\n",
    "    )\n",
    "\n",
    "print(sp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff778cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6884bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAME                                  ID              SIZE      MODIFIED     ',\n",
       " 'qwen3:4b                              2bfd38a7daaf    2.6 GB    23 hours ago    ',\n",
       " 'deepseek-r1:1.5b                      e0979632db5a    1.1 GB    23 hours ago    ',\n",
       " 'mxbai-embed-large:latest              468836162de7    669 MB    2 days ago      ',\n",
       " 'qwen2.5-coder:3b                      f72c60cabf62    1.9 GB    8 days ago      ',\n",
       " 'scb10x/typhoon2.1-gemma3-4b:latest    8cfab9097c9d    2.6 GB    4 weeks ago     ',\n",
       " 'bge-m3:latest                         790764642607    1.2 GB    5 weeks ago     ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_result.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8f157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen3:4b                              2bfd38a7daaf    2.6 GB    23 hours ago    ',\n",
       " 'deepseek-r1:1.5b                      e0979632db5a    1.1 GB    23 hours ago    ',\n",
       " 'mxbai-embed-large:latest              468836162de7    669 MB    2 days ago      ',\n",
       " 'qwen2.5-coder:3b                      f72c60cabf62    1.9 GB    8 days ago      ',\n",
       " 'scb10x/typhoon2.1-gemma3-4b:latest    8cfab9097c9d    2.6 GB    4 weeks ago     ',\n",
       " 'bge-m3:latest                         790764642607    1.2 GB    5 weeks ago     ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_result.splitlines()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fc0891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"aaaaaabbbbbbb\"\n",
    "word.index(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93603bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaaaa'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word[0: word.index('b')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6595d779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen3:4b',\n",
       " 'deepseek-r1:1.5b',\n",
       " 'mxbai-embed-large:latest',\n",
       " 'qwen2.5-coder:3b',\n",
       " 'scb10x/typhoon2.1-gemma3-4b:latest',\n",
       " 'bge-m3:latest']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = [model[0:model.index(' ')] for model in sp_result.splitlines()[1:]]\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7324a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\" : \"system\",\n",
    "        \"content\" : \"You are python code master\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"give me python code snippet by use huggingface library to call model 'scb10x/typhoon2.1-gemma3-4b' for chat. and only give me code\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1d0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da14ad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! To use the `transformers` library from Hugging Face, you need to install it first if you haven't already:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```\n",
       "\n",
       "Here's a basic example of how to call a model using the `scb10x/typhoon2.1-gemma3-4b` for chat:\n",
       "\n",
       "```python\n",
       "from transformers import pipeline\n",
       "\n",
       "# Load the chatbot pipeline\n",
       "chat_pipeline = pipeline(\"conversational\", model=\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "\n",
       "# Example conversation\n",
       "conversation_history = []\n",
       "\n",
       "while True:\n",
       "    user_input = input(\"User: \")\n",
       "    if user_input.lower() == \"exit\":\n",
       "        break\n",
       "\n",
       "    # Add user input to the conversation history\n",
       "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
       "\n",
       "    # Get assistant's response\n",
       "    response = chat_pipeline(\n",
       "        conversation_history,\n",
       "        max_length=100,  # Adjust this as needed for your use case\n",
       "        return_tensors=\"pt\"\n",
       "    )\n",
       "\n",
       "    # Add assistant's response to the conversation history\n",
       "    conversation_history.append({\"role\": \"assistant\", \"content\": response[0][\"generated_text\"]})\n",
       "\n",
       "    print(f\"Assistant: {response[0]['generated_text']}\")\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "- **`pipeline(\"conversational\")`**: This initializes a conversational pipeline, which handles text generation and understanding for dialogue.\n",
       "- **`model=\"scb10x/typhoon2.1-gemma3-4b\"`**: Specifies the model to use for generating responses.\n",
       "- **`conversation_history`**: A list of dictionaries that keeps track of the conversation history, including both user and assistant messages.\n",
       "- **While loop**: Continuously prompts the user for input until they type \"exit\".\n",
       "- **`chat_pipeline`**: Calls the pipeline with the current conversation history to get a response.\n",
       "- **`max_length=100`**: Limits the generated text length to 100 tokens. Adjust this based on your needs.\n",
       "\n",
       "This code provides a simple interactive chatbot using the specified model. You can run it in an environment where `transformers` is installed, such as Jupyter Notebook or a Python script."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(model=\"qwen2.5-coder:3b\", messages=messages)\n",
    "\n",
    "md(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da7a1344",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task conversational, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the chatbot pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chat_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversational\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscb10x/typhoon2.1-gemma3-4b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example conversation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/learnml/lib/python3.10/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    889\u001b[0m             class_ref,\n\u001b[1;32m    890\u001b[0m             model,\n\u001b[1;32m    891\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    892\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    893\u001b[0m         )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/learnml/lib/python3.10/site-packages/transformers/pipelines/__init__.py:548\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict, Any]:\n\u001b[1;32m    504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/learnml/lib/python3.10/site-packages/transformers/pipelines/base.py:1513\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1513\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task conversational, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the chatbot pipeline\n",
    "chat_pipeline = pipeline(\"conversational\", model=\"scb10x/typhoon2.1-gemma3-4b\")\n",
    "\n",
    "# Example conversation\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user input to the conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Get assistant's response\n",
    "    response = chat_pipeline(\n",
    "        conversation_history,\n",
    "        max_length=100,  # Adjust this as needed for your use case\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Add assistant's response to the conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response[0][\"generated_text\"]})\n",
    "\n",
    "    print(f\"Assistant: {response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356da3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, so I need to create a Python code snippet using the HuggingFace library to call a specific model. The model in question is 'scb10x/typhoon2.1-gemma3-4b'. \n",
       "\n",
       "First, I should figure out which module from HuggingFace corresponds to this exact string. I remember that each model name usually has a short version followed by a prefix like 'x', 'g', etc., but sometimes it's the full model without such prefixes. So I need to check if this model is in a public repository under its exact name.\n",
       "\n",
       "I can start by searching for the exact model name: \"scb10x/typhoon2.1-gemma3-4b\". Once I find or create that file, I'll use the `transformers.HfHubFileConfig` class to set up the configuration. \n",
       "\n",
       "Wait, but sometimes the full model without prefixes is what we need. Maybe 'typhoon2.1-gemma3-4b' suffices? So perhaps I should check both possibilities: using just that string and adding an 'x' suffix.\n",
       "\n",
       "Let me try creating the config with the short name first:\n",
       "```python\n",
       "from transformers import HfHubFileConfig\n",
       "\n",
       "config = HfHubFileConfig(\n",
       "    model_path=\"scb10x/typhoon2.1-gemma3-4b\"\n",
       ")\n",
       "```\n",
       "\n",
       "But if I create it without the 'x' suffix, it might not find the file because other researchers might have used that exact name. So I should also try that:\n",
       "```python\n",
       "config = HfHubFileConfig(\n",
       "    model_path=\"typhoon2.1-gemma3-4b\"\n",
       ")\n",
       "```\n",
       "\n",
       "I'll need to ensure that both paths are accessible so that the code runs without issues.\n",
       "\n",
       "After setting up the config, I can access the model using `config.get_model()`. This gives me access to all relevant information about the model's configuration and output channels, which is helpful for any additional processing or debugging.\n",
       "\n",
       "So putting it all together, my code should:\n",
       "\n",
       "1. Import HuggingFace modules.\n",
       "2. Create a configuration object with both possible paths (short and full).\n",
       "3. Access the model using `config.get_model()`.\n",
       "\n",
       "I should make sure that these imports are correct. Let me double-check:\n",
       "- From transformers import HfHubFileConfig\n",
       "- Import str for any error messages if the path doesn't exist.\n",
       "\n",
       "If I encounter any errors, it could be due to missing dependencies or incorrect paths. Once those issues are resolved, the code should function correctly.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(model=\"deepseek-r1:1.5b\", messages=messages, think=True)\n",
    "\n",
    "md(resp.message.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75de7401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To call the model 'scb10x/typhoon2.1-gemma3-4b' using HuggingFace in Python, follow these steps:\n",
       "\n",
       "```python\n",
       "from transformers import HfHubFileConfig\n",
       "\n",
       "# Create configuration object with short name\n",
       "config = HfHubFileConfig(\n",
       "    model_path=\"scb10x/typhoon2.1-gemma3-4b\"\n",
       ")\n",
       "\n",
       "# Or for full model without 'x' suffix, check if exists and create config\n",
       "if not hasattr(config, \"short_name\"):\n",
       "    # If the exact name is available elsewhere, use that\n",
       "    config = HfHubFileConfig(\n",
       "        model_path=\"typhoon2.1-gemma3-4b\"\n",
       "    )\n",
       "\n",
       "# Access the model configuration\n",
       "model_config = config.get_model()\n",
       "print(\"Model:\", model_config)\n",
       "```\n",
       "\n",
       "This code sets up a configuration for your model and retrieves its details, which can be useful for debugging or further processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ba1c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HfHubFileConfig' from 'transformers' (/Users/lawansiri/miniconda3/envs/learnml/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfHubFileConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create configuration object with short name\u001b[39;00m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m HfHubFileConfig(\n\u001b[1;32m      5\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscb10x/typhoon2.1-gemma3-4b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HfHubFileConfig' from 'transformers' (/Users/lawansiri/miniconda3/envs/learnml/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import HfHubFileConfig\n",
    "\n",
    "# Create configuration object with short name\n",
    "config = HfHubFileConfig(\n",
    "    model_path=\"scb10x/typhoon2.1-gemma3-4b\"\n",
    ")\n",
    "\n",
    "# Or for full model without 'x' suffix, check if exists and create config\n",
    "if not hasattr(config, \"short_name\"):\n",
    "    # If the exact name is available elsewhere, use that\n",
    "    config = HfHubFileConfig(\n",
    "        model_path=\"typhoon2.1-gemma3-4b\"\n",
    "    )\n",
    "\n",
    "# Access the model configuration\n",
    "model_config = config.get_model()\n",
    "print(\"Model:\", model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0716ff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, the user wants a Python code snippet that uses the Hugging Face library to call the model 'scb10x/typhoon2.1-gemma3-4b' for chat. Let me think about how to approach this.\n",
       "\n",
       "First, I need to check if this model is available on Hugging Face's model hub. The name 'scb10x/typhoon2.1-gemma3-4b' sounds like it might be a custom or less common model. I should verify if it's actually there. But since the user is asking for code, maybe they have access to it or it's a valid model.\n",
       "\n",
       "Next, the Hugging Face library for inference is usually HF Transformers. So I'll need to use the pipeline function from transformers. But wait, the model might not be available as a pipeline. Alternatively, maybe using the AutoModelForCausalLM and AutoTokenizer.\n",
       "\n",
       "Wait, the user said \"call model for chat\", so it's a chat model. The model might be a chat model, so using the pipeline with chat template. But I need to make sure that the model is compatible with the pipeline.\n",
       "\n",
       "Alternatively, maybe the model is a text generation model, so using the pipeline with text generation. Let me think about the steps:\n",
       "\n",
       "1. Install the necessary libraries: transformers, torch, and maybe accelerate if needed.\n",
       "\n",
       "2. Load the tokenizer and model. Since it's a 4B model, maybe it's a large model, so using the AutoModelForCausalLM and AutoTokenizer.\n",
       "\n",
       "3. Use the model for text generation. So, the code would be something like:\n",
       "\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import torch\n",
       "\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "model = AutoModelForCausalLM.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "\n",
       "input_text = \"Hello, how can I help you today?\"\n",
       "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
       "outputs = model.generate(**inputs)\n",
       "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "print(response)\n",
       "\n",
       "But wait, the model might require some specific settings. For example, the chat template. If the model is a chat model, maybe using the pipeline with chat_template. But if it's a generic text generation model, then the above code would work.\n",
       "\n",
       "Alternatively, using the pipeline:\n",
       "\n",
       "from transformers import pipeline\n",
       "\n",
       "pipe = pipeline(\"text-generation\", model=\"scb10x/typhoon2.1-gemma3-4b\", tokenizer=\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "response = pipe(\"Hello, how can I help you today?\")[0]['generated_text']\n",
       "print(response)\n",
       "\n",
       "But I need to check if the model is available in the pipeline. However, sometimes the pipeline might not work if the model isn't properly set up. Also, the model might be a large one, so maybe it's better to use the AutoModelForCausalLM and tokenizer directly.\n",
       "\n",
       "Another thing: the user might need to have internet access to download the model. So the code should include the necessary imports and the correct model name.\n",
       "\n",
       "Wait, the model name 'scb10x/typhoon2.1-gemma3-4b' – I'm not sure if that's a real model. Maybe it's a typo or a custom model. But the user is asking for code, so I'll proceed with that name.\n",
       "\n",
       "So the code would be:\n",
       "\n",
       "Install the libraries first. But the user might have them already. So the code would be:\n",
       "\n",
       "import torch\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "\n",
       "# Load tokenizer and model\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "model = AutoModelForCausalLM.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "\n",
       "# Input text\n",
       "input_text = \"Hello, how can I help you today?\"\n",
       "\n",
       "# Tokenize input\n",
       "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
       "\n",
       "# Generate response\n",
       "outputs = model.generate(**inputs, max_new_tokens=100)\n",
       "\n",
       "# Decode the output\n",
       "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "\n",
       "print(\"Model response:\", response)\n",
       "\n",
       "But I need to make sure that the model is compatible with the tokenizer. Also, the generate function might need some parameters like max_new_tokens, do_sample, etc. The user might need to adjust those.\n",
       "\n",
       "Alternatively, using the pipeline approach. But if the model is not in the pipeline, maybe the code would throw an error. So maybe the first approach is safer.\n",
       "\n",
       "Another thing: if the model is a chat model, maybe it uses a specific chat template. So the tokenizer might have a chat template. But in that case, the code would need to use the tokenizer's chat_template. But the user didn't mention that, so maybe the code is straightforward.\n",
       "\n",
       "So the final code would be as above. But I should also mention that the user needs to have the model downloaded, and that the model might be large, so it might take time and resources.\n",
       "\n",
       "But the user just wants the code, so I'll present the code as is, with the necessary imports and the steps to generate the response.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(model=\"qwen3:4b\", messages=messages, think=True)\n",
    "\n",
    "md(resp.message.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9385105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import torch\n",
       "\n",
       "# Load tokenizer and model\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "model = AutoModelForCausalLM.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
       "\n",
       "# Input text\n",
       "input_text = \"Hello, how can I help you today?\"\n",
       "\n",
       "# Tokenize input\n",
       "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
       "\n",
       "# Generate response\n",
       "outputs = model.generate(**inputs, max_new_tokens=100)\n",
       "\n",
       "# Decode the output\n",
       "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "\n",
       "print(\"Model response:\", response)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e254b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211c26925bfb4b4c8216fb67d024f8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15fcb8109ad44c9b2365832fde6c376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: Hello, how can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"scb10x/typhoon2.1-gemma3-4b\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Hello, how can I help you today?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Model response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
