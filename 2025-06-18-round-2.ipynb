{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7324a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\" : \"system\",\n",
    "        \"content\" : \"You are python code master\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"give me python code snippet by use huggingface library to call model 'scb10x/typhoon2.1-gemma3-4b' for chat. and only give me code\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1d0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da14ad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! To call a Hugging Face model using the `transformers` library for a chat-like interaction, you can follow these steps. Below is a simple example of how to do this:\n",
       "\n",
       "First, ensure you have the necessary libraries installed. You can install them using pip if you haven't already:\n",
       "\n",
       "```bash\n",
       "pip install transformers torch\n",
       "```\n",
       "\n",
       "Now, here's a code snippet that calls the 'scb10x/typhoon2.1-gemma3-4b' model for chat-like interaction:\n",
       "\n",
       "```python\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "\n",
       "# Load the tokenizer and model\n",
       "tokenizer = AutoTokenizer.from_pretrained('scb10x/typhoon2.1-gemma3-4b')\n",
       "model = AutoModelForCausalLM.from_pretrained('scb10x/typhoon2.1-gemma3-4b')\n",
       "\n",
       "# Function to generate chat-like responses\n",
       "def chat(prompt, max_length=512):\n",
       "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
       "    outputs = model.generate(inputs, max_length=max_length)\n",
       "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "    return response\n",
       "\n",
       "# Example usage\n",
       "prompt = \"Hello, how are you?\"\n",
       "response = chat(prompt)\n",
       "print(response)\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "- **Tokenizer and Model**: We use `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library to load the pre-trained model and tokenizer.\n",
       "- **Chat Function**: The `chat` function takes a prompt as input, encodes it using the tokenizer, generates a response using the model, and then decodes it back to text.\n",
       "- **Max Length**: You can adjust the `max_length` parameter to control the length of the generated responses. The default is set to 512 tokens.\n",
       "\n",
       "This code snippet provides a basic framework for interacting with a causal language model like 'scb10x/typhoon2.1-gemma3-4b'. Adjust the parameters as needed for your specific use case."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(model=\"qwen2.5-coder:3b\", messages=messages)\n",
    "\n",
    "md(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('scb10x/typhoon2.1-gemma3-4b')\n",
    "model = AutoModelForCausalLM.from_pretrained('scb10x/typhoon2.1-gemma3-4b')\n",
    "\n",
    "# Function to generate chat-like responses\n",
    "def chat(prompt, max_length=512):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, how are you?\"\n",
    "response = chat(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
